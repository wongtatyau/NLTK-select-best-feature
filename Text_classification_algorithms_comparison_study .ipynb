{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "import pprint\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "\n",
    "## Function to create feature set:\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "## Function to read all text file:\n",
    "def read_file(x):\n",
    "    with open(x,'r') as l:\n",
    "        lines = l.readlines()\n",
    "        list = []\n",
    "        for i in lines:\n",
    "            list.append(i)\n",
    "    return list\n",
    "\n",
    "# Gather all negative text fiel into a long list. \n",
    "list_n = [read_file(<your file path 1> + read_file(<your file path 2> + ...))]\n",
    "\n",
    "# Gather all positive text file into a long list:\n",
    "list_p = [read_file(<your file path 1> + read_file(<your file path 2> + ...))]\n",
    "\n",
    "#remove all the duplicated list \n",
    "list_neg = list(itertools.chain(*list_n))\n",
    "list_pos = list(itertools.chain(*list_p))\n",
    "\n",
    "# Create assignment for stop-word removal, lemmatisation and tokenisation. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Function to tokenisation, lowercasing and stop words removal. \n",
    "def list_keywords(ex_list):\n",
    "    exp = []\n",
    "    for i in ex_list:\n",
    "        token = tokenizer.tokenize(i.lower())\n",
    "        token = [t for t in token if t not in stop_words]\n",
    "        exp.append(token)\n",
    "    return(list(itertools.chain(*exp)))\n",
    "\n",
    "# Create keywords set.\n",
    "key_neg = list_keywords(list_neg)\n",
    "key_pos = list_keywords(list_pos)\n",
    "#Combine them and select the top 1000 keywords as word features. \n",
    "all_words = key_neg + key_pos\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:1000]\n",
    "\n",
    "# Function that return a sentence after tokenisation, stop-words removal, lemmatisation and ngrams inclusion. \n",
    "def list_trans(ex_list):\n",
    "    exp = []\n",
    "    for w in ex_list:\n",
    "        token = tokenizer.tokenize(w.lower())\n",
    "        token = [t for t in token if t not in stop_words]\n",
    "        token = [lemmatizer.lemmatize(w) for w in token ]\n",
    "        list_neg.append(ngrams(token,2))\n",
    "        list_neg.append(ngrams(token,3))\n",
    "        token = \" \".join(token)\n",
    "        exp.append(token)\n",
    "    return(exp)\n",
    "\n",
    "#Create comments set.\n",
    "list_negt = list_trans(list_neg)\n",
    "list_post = list_trans(list_pos)\n",
    "# Combined it and shuffle it randomly. \n",
    "combined_list_train = list_negt + list_post\n",
    "random.shuffle(combined_list_train)\n",
    "\n",
    "# Function for all classifier to compare based on 1) accurary rate & 2) processing time.\n",
    "def all_classifier():\n",
    "    for traincv, testcv in cv:\n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        e = []\n",
    "        f = []\n",
    "        g = []\n",
    "\n",
    "        t1 = []\n",
    "        t2 = []\n",
    "        t3 = []\n",
    "        t4 = []\n",
    "        t5 = []\n",
    "        t6 = []\n",
    "        t7 = []\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        a.append(nltk.classify.util.accuracy(classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t1.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "        MNB_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        b.append(nltk.classify.util.accuracy(MNB_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t2.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "        BernoulliNB_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        c.append(nltk.classify.util.accuracy(BernoulliNB_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t3.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "        LogisticRegression_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        d.append(nltk.classify.util.accuracy(LogisticRegression_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t4.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "        SGDClassifier_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        e.append(nltk.classify.util.accuracy(SGDClassifier_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t5.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        SVC_classifier = SklearnClassifier(SVC())\n",
    "        SVC_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        f.append(nltk.classify.util.accuracy(SVC_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t6.append(stop-start)\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "        LinearSVC_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "        g.append(nltk.classify.util.accuracy(LinearSVC_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "        stop = timeit.default_timer()\n",
    "        t7.append(stop-start)\n",
    "\n",
    "    nai.append(np.mean(a))\n",
    "    mul.append(np.mean(b))\n",
    "    ber.append(np.mean(c))\n",
    "    log.append(np.mean(d))\n",
    "    sto.append(np.mean(e))\n",
    "    sup.append(np.mean(f))\n",
    "    lin.append(np.mean(g))\n",
    "\n",
    "    tnai.append(np.mean(t1))\n",
    "    tmul.append(np.mean(t2))\n",
    "    tber.append(np.mean(t3))\n",
    "    tlog.append(np.mean(t4))\n",
    "    tsto.append(np.mean(t5))\n",
    "    tsup.append(np.mean(t6))\n",
    "tlin.append(np.mean(t7))\n",
    "\n",
    "for w in range(1,2864,10):\n",
    "    word_features = list(all_keys.keys())[:2000]\n",
    "    # Creating training and testing set\n",
    "    train_set = [(find_features(tokenizer.tokenize(rev)), category) for (rev, category) in combined_list_train]\n",
    "\n",
    "    ## Applying 10-fold cross validation.\n",
    "    cv = cross_validation.KFold(len(train_set), n_folds=10, shuffle=True)\n",
    "\n",
    "    all_classifier()\n",
    "    print(\"percentage of progress:\",(w/2864)*100)\n",
    "\n",
    "# Saving all your output using pickle. We are only saving accuracy rate and processing time for one classifier here. \n",
    "pickle_out = open(\"lin_frequency_filtered.pickle\",\"wb\")\n",
    "pickle.dump(lin, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"tlin_frequency_filtered.pickle\",\"wb\")\n",
    "pickle.dump(tlin, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# Done! You may now view your classifier results using Matplotlib. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
