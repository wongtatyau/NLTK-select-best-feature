{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "# Lock/block other thread from coming in while running it. \n",
    "print_lock = threading.Lock()\n",
    "\n",
    "# Defining an example jobs. \n",
    "def exampleJob(worker):\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    with print_lock:\n",
    "        print(threading.current_thread().name, worker)\n",
    "\n",
    "# Get the thread and workers \n",
    "def threader():\n",
    "    while True:\n",
    "        worker = q.get()\n",
    "        #Ask thread to do the job. (Assigned up there)\n",
    "        exampleJob(worker)\n",
    "        #Let comp knows the thread is available again and back to work\n",
    "        q.task_done()\n",
    " \n",
    "#Start using the function\n",
    "q = Queue()\n",
    "\n",
    "#Creating 10 tread\n",
    "for x in range(10):\n",
    "    #Set the target so as to run through \"threader\" (defined up there)\n",
    "    t = threading.Thread(target = threader)\n",
    "    #It means when all jobs are done, all thread will dead and killed by daemon! XDD\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for worker in range(20):\n",
    "        q.put(worker)\n",
    "\n",
    "# Put back all workers at the end and kill everything. \n",
    "q.join()\n",
    "\n",
    "print('Entire job took',time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "for w in range(20,30):\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3182f57058a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m           read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\BUSINESS_list_p.txt')]\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mlist_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlist_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[0mlist_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlist_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lenght of positive comments:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "import pprint\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# style.use('fivethirtyeight')\n",
    "# fig = plt.figure()\n",
    "\n",
    "#create features\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "def read_file(x):\n",
    "    with open(x,'r') as l:\n",
    "        lines = l.readlines()\n",
    "        list = []\n",
    "        for i in lines:\n",
    "            list.append(i)\n",
    "    return list\n",
    "\n",
    "\n",
    "list_n = [read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\rooms_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\VALUE_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\CLEANLINESS_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\VALUE_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\SERVICE_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\LOCATION_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\CHECKIN_list_n.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\neg\\BUSINESS_list_n.txt')]\n",
    "\n",
    "list_p = [read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\rooms_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\VALUE_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\CLEANLINESS_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\VALUE_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\SERVICE_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\LOCATION_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\CHECKIN_list_p.txt')+\n",
    "          read_file(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\training\\rooms\\pos\\BUSINESS_list_p.txt')]\n",
    "\n",
    "list_neg = list(itertools.chain(*list_n))\n",
    "list_pos = list(itertools.chain(*list_p))\n",
    "print(\"lenght of positive comments:\",len(list_pos))\n",
    "print(\"lenght of negative comments:\",len(list_neg))\n",
    "\n",
    "# Create n-grams (confirm correct) & stop-word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def list_keywords(ex_list):\n",
    "    exp = []\n",
    "    for i in ex_list:\n",
    "        token = tokenizer.tokenize(i.lower())\n",
    "        token = [t for t in token if t not in stop_words]\n",
    "        exp.append(token)\n",
    "    return(list(itertools.chain(*exp)))\n",
    "\n",
    "key_neg = list_keywords(list_neg)\n",
    "key_pos = list_keywords(list_pos)\n",
    "print(\"lenght of negative keywords:\",len(key_neg))\n",
    "print(\"lenght of positive keywords:\",len(key_pos))\n",
    "\n",
    "def list_trans(ex_list):\n",
    "    exp = []\n",
    "    for w in ex_list:\n",
    "        token = tokenizer.tokenize(w.lower())\n",
    "        token = [t for t in token if t not in stop_words]\n",
    "        # token = [lemmatizer.lemmatize(w) for w in token ]\n",
    "        # list_neg.append(ngrams(token,2))\n",
    "        # list_neg.append(ngrams(token,3))\n",
    "        token = \" \".join(token)\n",
    "        exp.append(token)\n",
    "    return(exp)\n",
    "\n",
    "list_negt = list_trans(list_neg)\n",
    "list_post = list_trans(list_pos)\n",
    "print('lenght of tokenized positive comments:',len(list_post))\n",
    "print('lenght of tokenized negative comments:',len(list_negt))\n",
    "\n",
    "list_negt = [(i,\"neg\") for i in list_negt]\n",
    "list_post = [(i,\"pos\") for i in list_post]\n",
    "# list_pos_test = [(i,'pos') for i in list_pos_test]\n",
    "# list_neg_test = [(i,'neg') for i in list_neg_test]\n",
    "\n",
    "combined_list_train = list_negt + list_post\n",
    "random.shuffle(combined_list_train)\n",
    "# combined_list_test = list_pos_test + list_neg_test\n",
    "\n",
    "\n",
    "# all_list = list_n + list_p\n",
    "# tf = TfidfVectorizer(analyzer='word',ngram_range=(1,2), min_df = 0, stop_words = 'english',lowercase=True)\n",
    "#\n",
    "# ## Forming features vectors\n",
    "# tfidf_matrix = tf.fit_transform(all_list)\n",
    "# feature_names = tf.get_feature_names()\n",
    "# # print(\"total number of features/variables is:\",len(feature_names))\n",
    "#\n",
    "# train_data_features = tfidf_matrix.toarray()\n",
    "# # print(\"total (sentence, variable) set is:\",train_data_features.shape)\n",
    "#\n",
    "# indices = np.argsort(tf.idf_)[::-1]\n",
    "# features = tf.get_feature_names()\n",
    "\n",
    "# test with most important features first.\n",
    "\n",
    "# key_pos = nltk.FreqDist(key_pos)\n",
    "# key_neg = nltk.FreqDist(key_neg)\n",
    "# print('no. of new key_pos:',len(key_pos))\n",
    "# print('no. of new key_neg:',len(key_neg))\n",
    "# key_pos = list(key_pos.keys())[:500]\n",
    "# key_neg = list(key_neg.keys())[:500]\n",
    "# word_features = key_neg + key_pos\n",
    "\n",
    "all_words = key_neg + key_pos\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print('total number of keywords:',len(all_words))\n",
    "word_features = list(all_words.keys())[:1000]\n",
    "\n",
    "# Creating training and testing set\n",
    "train_set = [(find_features(tokenizer.tokenize(rev)), category) for (rev, category) in combined_list_train]\n",
    "\n",
    "# top_features = [features[i] for i in indices[:500]]\n",
    "\n",
    "## 10-fold cross validation. \n",
    "cv = cross_validation.KFold(len(train_set), n_folds=10, shuffle=False)\n",
    "\n",
    "nai=[]\n",
    "mul = []\n",
    "ber = []\n",
    "log = []\n",
    "sto = []\n",
    "sup = []\n",
    "lin = []\n",
    "\n",
    "for traincv, testcv in cv:\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    nai.append(nltk.classify.util.accuracy(classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "    MNB_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    mul.append(nltk.classify.util.accuracy(MNB_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "    BernoulliNB_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    ber.append(nltk.classify.util.accuracy(BernoulliNB_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "    LogisticRegression_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    log.append(nltk.classify.util.accuracy(LogisticRegression_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "    SGDClassifier_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    sto.append(nltk.classify.util.accuracy(SGDClassifier_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    SVC_classifier = SklearnClassifier(SVC())\n",
    "    SVC_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    sup.append(nltk.classify.util.accuracy(SVC_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "    LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "    LinearSVC_classifier.train(train_set[traincv[0]:traincv[len(traincv)-1]])\n",
    "    lin.append(nltk.classify.util.accuracy(LinearSVC_classifier, train_set[testcv[0]:testcv[len(testcv)-1]]))\n",
    "\n",
    "print(nai)\n",
    "print(mul)\n",
    "print(ber)\n",
    "print(log)\n",
    "print(sto)\n",
    "print(sup)\n",
    "print(lin)\n",
    "\n",
    "print(numpy.mean(nai))\n",
    "print(numpy.mean(mul))\n",
    "print(numpy.mean(ber))\n",
    "print(numpy.mean(log))\n",
    "print(numpy.mean(sto))\n",
    "print(numpy.mean(sup))\n",
    "print(numpy.mean(lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "reader = csv.read(open(r'C:\\Users\\wongtattat\\Desktop\\hotel_reviews\\IRIS.txt,'r'))\n",
    "\n",
    "def gen_chunks(reader, chunksize=10):\n",
    "    chunk = []\n",
    "    for i, line in enumerate(reader):\n",
    "        if (i % chunksize == 0 and i > 0):\n",
    "            yield chunk\n",
    "            del chunk[:]\n",
    "        chunk.append(line)\n",
    "    yield chunk\n",
    "\n",
    "for chunk in gen_chunk(reader):\n",
    "    print(chunk)\n",
    "                       \n",
    "\n",
    "                       \n",
    "                       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Esc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bd6cd44d333c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEsc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Esc' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
